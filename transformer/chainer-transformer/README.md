# chainer-transformer

This repository contains an example implementation of the transformer, 
as published in the [Attention is all you need](https://arxiv.org/abs/1706.03762) paper.
This implementation is based on the PyTorch implementation by 
[OpenNMT](http://nlp.seas.harvard.edu/2018/04/03/attention.html).

The code for the transformer can be found in the directory `transformer`.
Apart from the transformer code, this repository contains data loading and evaluation code.
